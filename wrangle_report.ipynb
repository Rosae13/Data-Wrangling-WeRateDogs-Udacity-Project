{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling means to round up, herd, or take charge of livestock, like horses or sheep. Wrangling is actually a weird word.\n",
    "Data Wrangling means gathering, assessing, and cleaning data.\n",
    "\n",
    "\n",
    "Why Wrangle Data?\n",
    "\n",
    "Data wrangling is a core skill that everyone who works with data should be familiar with since so much of the world's data is not clean. \n",
    "\n",
    "Data Wrangling process is divided into three main steps:\n",
    "\n",
    "• Gathering.\n",
    "• Assessing.\n",
    "• Cleaning.\n",
    "\n",
    "This wrangle report is part of the Wrangle and Analyze Data project in order to document the wrangling efforts of the project. The dataset used in this project is the tweet archive of Twitter user@dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people’s dogs with humorous comment about the dog. We have to wrangle the data through the following process of data wrangling:\n",
    "\n",
    "\n",
    "• Gathering:- Data was gathered from three different sources:\n",
    "1. WeRateDogs Twitter archive given by Udacity in csv format: \n",
    "Data was gathered from WeRateDogs Twitter archive given by Udacity in csv format. Using the Pandas method 'read_csv', to read the stored data in the file 'twitter-archive-enhanced.csv' and thereafter stored it in a DataFrame called 'twitter_archives'.\n",
    "2. Image prediction file downloaded programmatically using Requests library and the URL provided by Udacity in tsv format:\n",
    "Using Requests library and 'get' method, data was downloaded in a file 'image_predictions.tsv'. Then, the content was stored in a DataFrame called 'image_predictions'  using the Pandas method 'read_csv'.\n",
    "3. Twitter's APIs Using Tweepy library: \n",
    "Using the Requests library to download the tweet JSON file programmatically since my application for twitter APIs key on Developer Account wasn't approved.\n",
    "\n",
    "• Assessing: \n",
    "After gathering the data and storing them in DataFrames, the next step was assessing the data for quality and tidiness. Data were assessed programmatically and visually.\n",
    "1. Quality:\n",
    "Issues with content. Low quality data is also known as dirty data. The identified quality issues are:\n",
    "  a.  Some missing values in the column. Remove retweets: It is only original tweets we need.\n",
    "\n",
    "  b.  Name column contain false names like \"a\".\n",
    "\n",
    "  c.  Underscores in p1, p2 and p3 column\n",
    "  \n",
    "  d.  Inconsistent capitalization in p1, p2 and p3 columns\n",
    "  \n",
    "  e.  False predictions: predictions contain many entries that are not dogs , e.g., soccer_ball, cardigan, stove, pot,\n",
    "     mailbox, shovel, banana, etc.\n",
    "\n",
    "  f.  The proportions in p1_conf, p2_conf and p3_conf columns should be percentages\n",
    "\n",
    "  g.  The numerator and denominator columns have invalid values and rating containing decimal numbers in numerator\n",
    "  \n",
    "  h.  Most predicted breed for each prediction level should be created.\n",
    "\n",
    "  i.  Some column headers are not descriptive e.g jpg_url\n",
    "  \n",
    "  j.  Sources format are not readable\n",
    "  \n",
    "  k.  timestamp is in object instead of string and datetime and date and time should be separated\n",
    "\n",
    "  l.  Remove duplicated columns\n",
    "\n",
    "  m.  Some columns are in inappropriate data type e.g tweet_id should be a string not an integer \n",
    "2. Tidiness:\n",
    "Issues with structure that prevent easy analysis. Untidy data is also known as messy data. The dentified tidiness issues are:\n",
    "\n",
    "  a. Dog type(doggo, floofer, pupper, puppo) should be in one column\n",
    "\n",
    "  b. The three tables(twitter_archive, image_predictions and tweet_json) should be merged into one since they're all\n",
    "     related to the same type of observational unit according to tidy data requirements\n",
    "     \n",
    "• Cleaning:\n",
    "It is the process of fixing and resolving issues identified in the Cleaning process. The (define, code, and test) steps were used in the cleaning process. First, copies of the DataFrames were created and merged before cleaning. Then, the steps of cleaning were applied iteratively on all issues.\n",
    "\n",
    "Other Data Wrangling Process include:\n",
    "\n",
    "• Storing:\n",
    "The final merged DataFrame called 'twitter_dogs_data' contains 1611 rows and 16 columns with the correct data types. The dataset is then stored in a csv file called 'twitter_archive_master.csv'. At this point, the data was successfully wrangled and therefore ready for analysis and visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
